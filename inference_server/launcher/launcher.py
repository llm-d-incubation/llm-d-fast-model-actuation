# Copyright 2025 The llm-d Authors.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# 	http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
vLLM Launcher
"""

import logging
import multiprocessing
import os
import queue
import signal
import sys
import uuid
from contextlib import asynccontextmanager
from http import HTTPStatus  # HTTP Status Codes
from typing import Any, Dict, List, Optional

import uvloop
from fastapi import FastAPI, HTTPException, Path, Query
from fastapi.responses import JSONResponse
from gputranslator import GpuTranslator
from pydantic import BaseModel
from vllm.entrypoints.openai.api_server import run_server
from vllm.entrypoints.openai.cli_args import make_arg_parser, validate_parsed_serve_args
from vllm.entrypoints.utils import cli_env_setup
from vllm.utils.argparse_utils import FlexibleArgumentParser

# Queue size limits
MAX_QUEUE_SIZE = 5000  # Maximum number of log messages in queue
MAX_LOG_RESPONSE_BYTES = 1 * 1024 * 1024  # 1 MB default for API response


class LogRangeNotAvailable(Exception):
    """Raised when the requested start_byte is beyond available log content"""

    def __init__(self, start_byte, available_bytes):
        self.start_byte = start_byte
        self.available_bytes = available_bytes
        super().__init__(
            f"start_byte {start_byte} is beyond available content "
            f"({available_bytes} bytes available)"
        )


# Define a the expected JSON structure in dataclass
class VllmConfig(BaseModel):
    options: str
    gpu_uuids: Optional[List[str]] = None
    env_vars: Optional[Dict[str, Any]] = None


class VllmInstance:
    """Represents a single vLLM instance"""

    def __init__(
        self, instance_id: str, config: VllmConfig, gpu_translator: GpuTranslator
    ):
        """
        Initialize VllmInstance object
        :param instance_id: Instance id (autogenerated or custom)
        :param config: VllmConfig object
        :param gpu_translator: GpuTranslator object
        """

        # Check for CUDA device UUIDs and set CUDA_VISIBLE_DEVICES accordingly
        if config.gpu_uuids:
            cuda_indices = []
            for uuid_str in config.gpu_uuids:
                index = gpu_translator.uuid_to_index(uuid_str)
                cuda_indices.append(str(index))

            if config.env_vars is None:
                config.env_vars = {}
            config.env_vars["CUDA_VISIBLE_DEVICES"] = ",".join(cuda_indices)
            logger.info(
                f"Set CUDA_VISIBLE_DEVICES to \
                    {config.env_vars['CUDA_VISIBLE_DEVICES']} based on UUIDs."
            )

        # Initialize instance variables
        self.instance_id = instance_id
        self.config = config
        self.process: Optional[multiprocessing.Process] = None
        self.output_queue: Optional[multiprocessing.Queue] = None
        self._log_buffer: bytes = b""

    def start(self) -> dict:
        """
        Start this vLLM instance
        :return: Status of the process.
        """
        if self.process and self.process.is_alive():
            return {"status": "already_running", "instance_id": self.instance_id}

        self.output_queue = multiprocessing.Queue(maxsize=MAX_QUEUE_SIZE)
        self.process = multiprocessing.Process(
            target=vllm_kickoff, args=(self.config, self.output_queue)
        )
        self.process.start()

        return {
            "status": "started",
            "instance_id": self.instance_id,
        }

    def stop(self, timeout: int = 10) -> dict:
        """
        Stop existing vLLM instance
        :param timeout: waits for the process to stop, defaults to 10
        :return: a dictionary with the status "terminated"
        """
        if not self.process or not self.process.is_alive():
            return {
                "status": "not_running",
                "instance_id": self.instance_id,
            }

        # Graceful termination â€” send SIGTERM to the vLLM process,
        # which will propagate shutdown to the EngineCore via vLLM's
        # own cleanup logic.
        self.process.terminate()
        self.process.join(timeout=timeout)

        # Force kill the entire process group (vLLM server + EngineCore)
        # if graceful shutdown did not complete in time.
        if self.process.is_alive():
            try:
                os.killpg(self.process.pid, signal.SIGKILL)
            except ProcessLookupError:
                pass
            self.process.join()

        return {
            "status": "terminated",
            "instance_id": self.instance_id,
        }

    def get_status(self) -> dict:
        """
        Returns the status of the process
        :return: Status of the running process.
        """

        return {
            "status": "running" if self.process.is_alive() else "stopped",
            "instance_id": self.instance_id,
        }

    def _drain_queue_to_buffer(self):
        """Drain all pending messages from the queue into the persistent log buffer."""
        if not self.output_queue:
            return
        while not self.output_queue.empty():
            try:
                msg = self.output_queue.get_nowait()
                self._log_buffer += msg.encode("utf-8")
            except queue.Empty:
                break

    def get_logs(
        self, start_byte: int = 0, max_bytes: int = MAX_LOG_RESPONSE_BYTES
    ) -> str:
        """
        Retrieve logs from the child process.
        :param start_byte: Byte position to start reading from
        :param max_bytes: Maximum bytes of log data to retrieve
        :return: Log content as string
        :raises LogRangeNotAvailable: If start_byte is beyond available content
        """
        self._drain_queue_to_buffer()
        total_available = len(self._log_buffer)

        if start_byte > total_available:
            raise LogRangeNotAvailable(start_byte, total_available)

        end_byte = start_byte + max_bytes
        log_bytes = self._log_buffer[start_byte:end_byte]
        return log_bytes.decode("utf-8", errors="replace")


# Multi-instance vLLM process manager
class VllmMultiProcessManager:
    def __init__(self):
        self.instances: Dict[str, VllmInstance] = {}
        self.gpu_translator = GpuTranslator()

    def create_instance(
        self, vllm_config: VllmConfig, instance_id: Optional[str] = None
    ) -> dict:
        """Create and start a new vLLM instance"""
        if instance_id is None:
            instance_id = str(uuid.uuid4())

        if instance_id in self.instances:
            raise ValueError(f"Instance with ID {instance_id} already exists")

        instance = VllmInstance(instance_id, vllm_config, self.gpu_translator)
        self.instances[instance_id] = instance

        return instance.start()

    def stop_instance(self, instance_id: str, timeout: int = 10) -> dict:
        """Stop a specific vLLM instance"""
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")

        result = self.instances[instance_id].stop(timeout)

        # Clean up stopped instance
        if result["status"] in ["terminated", "not_running"]:
            del self.instances[instance_id]

        return result

    def stop_all_instances(self, timeout: int = 10) -> dict:
        """Stop all running vLLM instances"""
        results = []
        instance_ids = list(self.instances.keys())

        for instance_id in instance_ids:
            try:
                result = self.stop_instance(instance_id, timeout)
                results.append(result)
            except KeyError:
                continue  # Instance was already removed

        return {
            "status": "all_stopped",
            "stopped_instances": results,
            "total_stopped": len(results),
        }

    def get_instance_status(self, instance_id: str) -> dict:
        """Get status of a specific instance"""
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")

        return self.instances[instance_id].get_status()

    def get_all_instances_status(self) -> dict:
        """Get status of all instances"""
        instances_status = []
        running_count = 0

        for instance in self.instances.values():
            status = instance.get_status()
            instances_status.append(status)
            if status["status"] == "running":
                running_count += 1

        return {
            "total_instances": len(self.instances),
            "running_instances": running_count,
            "instances": instances_status,
        }

    def list_instances(self) -> List[str]:
        """List all instance IDs"""
        return list(self.instances.keys())

    def get_instance_logs(
        self,
        instance_id: str,
        start_byte: int = 0,
        max_bytes: int = MAX_LOG_RESPONSE_BYTES,
    ) -> str:
        """
        Get logs from a specific instance
        :param instance_id: ID of the instance
        :param start_byte: Byte position to start reading from
        :param max_bytes: Maximum bytes of log data to retrieve
        :return: Log content as string
        :raises LogRangeNotAvailable: If start_byte is beyond available content
        """
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")
        return self.instances[instance_id].get_logs(start_byte, max_bytes)


# Create global manager instance
vllm_manager = VllmMultiProcessManager()

# Setup logging
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(application: FastAPI):
    """Manage application lifecycle: clean up all vLLM instances on shutdown."""
    yield
    logger.info("Launcher shutting down, stopping all vLLM instances...")
    vllm_manager.stop_all_instances()


# Create FastAPI application
app = FastAPI(
    title="Multi-Instance vLLM Management API",
    version="2.0",
    description="REST API for managing multiple vLLM instances",
    lifespan=lifespan,
)


############################################################
# Health Endpoint
############################################################
@app.get("/health")
async def health():
    """Health Status"""
    return JSONResponse(content={"status": "OK"}, status_code=HTTPStatus.OK)


######################################################################
# GET INDEX
######################################################################
@app.get("/")
async def index():
    """Root URL response"""
    return JSONResponse(
        content={
            "name": "Multi-Instance vLLM Management API",
            "version": "2.0",
            "endpoints": {
                "index": "GET /",
                "health": "GET /health",
                "create_instance": "POST /v2/vllm/instances",
                "create_named_instance": "PUT /v2/vllm/instances/{instance_id}",
                "delete_instance": "DELETE /v2/vllm/instances/{instance_id}",
                "delete_all_instances": "DELETE /v2/vllm/instances",
                "get_instance_status": "GET /v2/vllm/instances/{instance_id}",
                "get_all_instances": "GET /v2/vllm/instances",
                "get_instance_logs": "GET /v2/vllm/instances/{instance_id}/log",
            },
        },
        status_code=HTTPStatus.OK,
    )


######################################################################
# vLLM MANAGEMENT ENDPOINTS
######################################################################
@app.post("/v2/vllm/instances")
async def create_vllm_instance(vllm_config: VllmConfig):
    """Create a new vLLM instance with random instance ID"""

    try:
        result = vllm_manager.create_instance(vllm_config)
        return JSONResponse(content=result, status_code=HTTPStatus.CREATED)
    except Exception as e:
        logger.error(f"Failed to create vLLM instance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/v2/vllm/instances/{instance_id}")
async def create_id_vllm_instance(
    vllm_config: VllmConfig,
    instance_id: str = Path(..., description="Custom instance ID"),
):
    """Create a new vLLM instance with instance ID"""
    try:
        result = vllm_manager.create_instance(vllm_config, instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.CREATED)
    except ValueError as e:
        raise HTTPException(status_code=409, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to create vLLM instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v2/vllm/instances/{instance_id}")
async def delete_vllm_instance(
    instance_id: str = Path(..., description="Instance ID to delete")
):
    """Delete a specific vLLM instance"""
    try:
        result = vllm_manager.stop_instance(instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")
    except Exception as e:
        logger.error(f"Failed to delete vLLM instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v2/vllm/instances")
async def delete_all_vllm_instances():
    """Delete all vLLM instances"""
    try:
        result = vllm_manager.stop_all_instances()
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except Exception as e:
        logger.error(f"Failed to delete all vLLM instances: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v2/vllm/instances")
async def get_all_vllm_instances(detail: bool = True):
    """
    Get information about all vLLM instances

    Query Parameters:
    - detail: If True (default), returns full status of all instances.
              If False, returns only instance IDs.
    """
    if detail:
        result = vllm_manager.get_all_instances_status()
    else:
        instances = vllm_manager.list_instances()
        result = {"instance_ids": instances, "count": len(instances)}

    return JSONResponse(content=result, status_code=HTTPStatus.OK)


@app.get("/v2/vllm/instances/{instance_id}")
async def get_vllm_instance_status(
    instance_id: str = Path(..., description="Instance ID")
):
    """Get status of a specific vLLM instance"""
    try:
        result = vllm_manager.get_instance_status(instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")


@app.get("/v2/vllm/instances/{instance_id}/log")
async def get_vllm_instance_logs(
    instance_id: str = Path(..., description="Instance ID"),
    start_byte: int = Query(
        0,
        description="Byte position to start reading from (0-based)",
        ge=0,
    ),
    max_bytes: int = Query(
        MAX_LOG_RESPONSE_BYTES,
        description="Maximum bytes of log data to retrieve",
        ge=1024,
        le=10 * 1024 * 1024,
    ),
):
    """
    Get logs from a specific vLLM instance starting from a byte position.

    Use start_byte=0 to read from the beginning.
    Use start_byte + len(log) in subsequent requests to continue reading.
    """
    try:
        log_content = vllm_manager.get_instance_logs(instance_id, start_byte, max_bytes)
        return JSONResponse(
            content={
                "log": log_content,
            },
            status_code=HTTPStatus.OK,
        )
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")
    except LogRangeNotAvailable as e:
        return JSONResponse(
            content={"available_bytes": e.available_bytes},
            status_code=HTTPStatus.REQUESTED_RANGE_NOT_SATISFIABLE,
            headers={"Content-Range": f"bytes */{e.available_bytes}"},
        )
    except Exception as e:
        logger.error(f"Failed to get logs for instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


######################################################################
# HELPER FUNCTIONS
######################################################################


# Helper class to redirect stdout/stderr to queue
class QueueWriter:
    """Custom writer that sends output to a multiprocessing Queue.

    Wraps the original stream so that libraries which inspect stream
    attributes (e.g. uvicorn's logging config) continue to work.
    """

    def __init__(self, output_queue: multiprocessing.Queue, original_stream):
        self._queue = output_queue
        self._original = original_stream
        # Expose attributes that logging/uvicorn may inspect
        self.encoding = getattr(original_stream, "encoding", "utf-8")
        self.name = getattr(original_stream, "name", "<queue>")
        self.errors = getattr(original_stream, "errors", "strict")

    def write(self, msg: str):
        if msg.strip():  # Only send non-empty messages
            try:
                self._queue.put_nowait(msg)
            except queue.Full:
                # Drop message if queue is full
                pass
        return len(msg)

    def flush(self):
        self._original.flush()

    def fileno(self):
        return self._original.fileno()

    def isatty(self):
        return False


# Function to be executed by the child process
def vllm_kickoff(vllm_config: VllmConfig, output_queue: multiprocessing.Queue):
    """
    Child function to kickoff vllm instance
    :param vllm_config: vLLM configuration parameters and env variables
    :param output_queue: multiprocessing Queue for capturing stdout/stderr
    """

    # Isolate this process tree into its own process group so that
    # signals (SIGINT/SIGTERM) sent to the launcher's group do not
    # propagate to the vLLM server or its EngineCore child process.
    os.setpgrp()

    # Redirect stdout and stderr to queue while preserving original
    # stream attributes needed by uvicorn's logging configuration.
    sys.stdout = QueueWriter(output_queue, sys.stdout)
    sys.stderr = QueueWriter(output_queue, sys.stderr)

    logger.info(f"VLLM process (PID: {os.getpid()}) started.")
    # Set env vars in the current process
    if vllm_config.env_vars:
        set_env_vars(vllm_config.env_vars)

    # prepare args
    receive_args = vllm_config.options.split()

    cli_env_setup()
    parser = FlexibleArgumentParser(
        description="vLLM OpenAI-Compatible RESTful API server."
    )
    parser = make_arg_parser(parser)
    args = parser.parse_args(receive_args)
    validate_parsed_serve_args(args)

    uvloop.run(run_server(args))


# Function to set env variables
def set_env_vars(env_vars: Dict[str, Any]):
    """
    Set environment variables from a dictionary
    :param env_vars: Dict with environment var name as keys and value as values
    """

    # Set environment variables from a dictionary
    for key, value in env_vars.items():
        os.environ[key] = str(value)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info")
