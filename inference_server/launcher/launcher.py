# Copyright 2025 The llm-d Authors.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# 	http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
vLLM Launcher
"""

import logging
import multiprocessing
import os
import queue
import sys
import uuid
from http import HTTPStatus  # HTTP Status Codes
from typing import Any, Dict, List, Optional

import uvloop
from fastapi import FastAPI, HTTPException, Path, Query
from fastapi.responses import JSONResponse
from gputranslator import GpuTranslator
from pydantic import BaseModel
from vllm.entrypoints.openai.api_server import run_server
from vllm.entrypoints.openai.cli_args import make_arg_parser, validate_parsed_serve_args
from vllm.entrypoints.utils import cli_env_setup
from vllm.utils.argparse_utils import FlexibleArgumentParser

# Queue size limits
MAX_QUEUE_SIZE = 5000  # Maximum number of log messages in queue
MAX_QUEUE_BYTES = 10 * 1024 * 1024  # 10 MB default for queue buffer
MAX_LOG_RESPONSE_BYTES = 1 * 1024 * 1024  # 1 MB default for API response


def get_logs_from_queue(
    output_queue: multiprocessing.Queue,
    start_byte: int = 0,
    max_bytes: int = MAX_LOG_RESPONSE_BYTES,
) -> tuple[str, int]:
    """
    Retrieve logs from queue starting from
    start_byte up to max_bytes (non-destructive read)
    :param output_queue: The multiprocessing queue containing log messages
    :param start_byte: Byte position to start reading from (0-based)
    :param max_bytes: Maximum bytes to retrieve from start_byte
    :return: Tuple of (log content as string, next_byte_position)
    """
    log_content = ""
    current_byte = 0
    bytes_read = 0
    temp_messages = []

    # Collect all messages from queue
    while not output_queue.empty():
        try:
            msg = output_queue.get_nowait()
            temp_messages.append(msg)
        except queue.Empty:
            break

    # Process messages: skip until start_byte, then collect up to max_bytes
    for msg in temp_messages:
        msg_bytes = len(msg.encode("utf-8"))
        msg_start_byte = current_byte
        msg_end_byte = current_byte + msg_bytes

        # Check if this message starts at or after our start position
        if msg_start_byte >= start_byte:
            # Check if adding this message would exceed max_bytes
            if bytes_read + msg_bytes <= max_bytes:
                log_content += msg
                bytes_read += msg_bytes

        current_byte = msg_end_byte

    # Put ALL messages back into the queue for future reads
    for msg in temp_messages:
        try:
            output_queue.put_nowait(msg)
        except queue.Full:
            # If queue is full, we've hit the size limit
            # Drop oldest messages by not re-queuing them
            break

    # Calculate next byte position for client to use
    next_byte = start_byte + bytes_read

    return log_content, next_byte


# Define a the expected JSON structure in dataclass
class VllmConfig(BaseModel):
    options: str
    gpu_uuids: Optional[List[str]] = None
    env_vars: Optional[Dict[str, Any]] = None


class VllmInstance:
    """Represents a single vLLM instance"""

    def __init__(
        self, instance_id: str, config: VllmConfig, gpu_translator: GpuTranslator
    ):
        """
        Initialize VllmInstance object
        :param instance_id: Instance id (autogenerated or custom)
        :param config: VllmConfig object
        :param gpu_translator: GpuTranslator object
        """

        # Check for CUDA device UUIDs and set CUDA_VISIBLE_DEVICES accordingly
        if config.gpu_uuids:
            cuda_indices = []
            for uuid_str in config.gpu_uuids:
                index = gpu_translator.uuid_to_index(uuid_str)
                cuda_indices.append(str(index))

            if config.env_vars is None:
                config.env_vars = {}
            config.env_vars["CUDA_VISIBLE_DEVICES"] = ",".join(cuda_indices)
            logger.info(
                f"Set CUDA_VISIBLE_DEVICES to \
                    {config.env_vars['CUDA_VISIBLE_DEVICES']} based on UUIDs."
            )

        # Initialize instance variables
        self.instance_id = instance_id
        self.config = config
        self.process: Optional[multiprocessing.Process] = None
        self.output_queue: Optional[multiprocessing.Queue] = None

    def start(self) -> dict:
        """
        Start this vLLM instance
        :return: Status of the process.
        """
        if self.process and self.process.is_alive():
            return {"status": "already_running", "instance_id": self.instance_id}

        self.output_queue = multiprocessing.Queue(maxsize=MAX_QUEUE_SIZE)
        self.process = multiprocessing.Process(
            target=vllm_kickoff, args=(self.config, self.output_queue)
        )
        self.process.start()

        return {
            "status": "started",
            "instance_id": self.instance_id,
        }

    def stop(self, timeout: int = 10) -> dict:
        """
        Stop existing vLLM instance
        :param timeout: waits for the process to stop, defaults to 10
        :return: a dictionary with the status "terminated"
        """
        if not self.process or not self.process.is_alive():
            return {
                "status": "not_running",
                "instance_id": self.instance_id,
            }

        # Graceful termination
        self.process.terminate()
        self.process.join(timeout=timeout)

        # Force kill if needed
        if self.process.is_alive():
            self.process.kill()
            self.process.join()

        return {
            "status": "terminated",
            "instance_id": self.instance_id,
        }

    def get_status(self) -> dict:
        """
        Returns the status of the process
        :return: Status of the running process.
        """

        return {
            "status": "running" if self.process.is_alive() else "stopped",
            "instance_id": self.instance_id,
        }

    def get_logs(
        self, start_byte: int = 0, max_bytes: int = MAX_LOG_RESPONSE_BYTES
    ) -> tuple[str, int]:
        """
        Retrieve logs from the child process
        :param start_byte: Byte position to start reading from
        :param max_bytes: Maximum bytes of log data to retrieve
        :return: Tuple of (log content as string, next_byte_position)
        """
        log_content = ""
        next_byte = start_byte
        if self.output_queue:
            log_content, next_byte = get_logs_from_queue(
                self.output_queue, start_byte, max_bytes
            )
        return log_content, next_byte


# Multi-instance vLLM process manager
class VllmMultiProcessManager:
    def __init__(self):
        self.instances: Dict[str, VllmInstance] = {}
        self.gpu_translator = GpuTranslator()

    def create_instance(
        self, vllm_config: VllmConfig, instance_id: Optional[str] = None
    ) -> dict:
        """Create and start a new vLLM instance"""
        if instance_id is None:
            instance_id = str(uuid.uuid4())

        if instance_id in self.instances:
            raise ValueError(f"Instance with ID {instance_id} already exists")

        instance = VllmInstance(instance_id, vllm_config, self.gpu_translator)
        self.instances[instance_id] = instance

        return instance.start()

    def stop_instance(self, instance_id: str, timeout: int = 10) -> dict:
        """Stop a specific vLLM instance"""
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")

        result = self.instances[instance_id].stop(timeout)

        # Clean up stopped instance
        if result["status"] in ["terminated", "not_running"]:
            del self.instances[instance_id]

        return result

    def stop_all_instances(self, timeout: int = 10) -> dict:
        """Stop all running vLLM instances"""
        results = []
        instance_ids = list(self.instances.keys())

        for instance_id in instance_ids:
            try:
                result = self.stop_instance(instance_id, timeout)
                results.append(result)
            except KeyError:
                continue  # Instance was already removed

        return {
            "status": "all_stopped",
            "stopped_instances": results,
            "total_stopped": len(results),
        }

    def get_instance_status(self, instance_id: str) -> dict:
        """Get status of a specific instance"""
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")

        return self.instances[instance_id].get_status()

    def get_all_instances_status(self) -> dict:
        """Get status of all instances"""
        instances_status = []
        running_count = 0

        for instance in self.instances.values():
            status = instance.get_status()
            instances_status.append(status)
            if status["status"] == "running":
                running_count += 1

        return {
            "total_instances": len(self.instances),
            "running_instances": running_count,
            "instances": instances_status,
        }

    def list_instances(self) -> List[str]:
        """List all instance IDs"""
        return list(self.instances.keys())

    def get_instance_logs(
        self,
        instance_id: str,
        start_byte: int = 0,
        max_bytes: int = MAX_LOG_RESPONSE_BYTES,
    ) -> tuple[str, int]:
        """
        Get logs from a specific instance
        :param instance_id: ID of the instance
        :param start_byte: Byte position to start reading from
        :param max_bytes: Maximum bytes of log data to retrieve
        :return: Tuple of (log content as string, next_byte_position)
        """
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")
        return self.instances[instance_id].get_logs(start_byte, max_bytes)


# Create global manager instance
vllm_manager = VllmMultiProcessManager()

# Create FastAPI application
app = FastAPI(
    title="Multi-Instance vLLM Management API",
    version="2.0",
    description="REST API for managing multiple vLLM instances",
)

# Setup logging
logger = logging.getLogger(__name__)


############################################################
# Health Endpoint
############################################################
@app.get("/health")
async def health():
    """Health Status"""
    return JSONResponse(content={"status": "OK"}, status_code=HTTPStatus.OK)


######################################################################
# GET INDEX
######################################################################
@app.get("/")
async def index():
    """Root URL response"""
    return JSONResponse(
        content={
            "name": "Multi-Instance vLLM Management API",
            "version": "2.0",
            "endpoints": {
                "index": "GET /",
                "health": "GET /health",
                "create_instance": "POST /v2/vllm/instances",
                "create_named_instance": "PUT /v2/vllm/instances/{instance_id}",
                "delete_instance": "DELETE /v2/vllm/instances/{instance_id}",
                "delete_all_instances": "DELETE /v2/vllm/instances",
                "get_instance_status": "GET /v2/vllm/instances/{instance_id}",
                "get_all_instances": "GET /v2/vllm/instances",
                "get_instance_logs": "GET /v2/vllm/instances/{instance_id}/log",
            },
        },
        status_code=HTTPStatus.OK,
    )


######################################################################
# vLLM MANAGEMENT ENDPOINTS
######################################################################
@app.post("/v2/vllm/instances")
async def create_vllm_instance(vllm_config: VllmConfig):
    """Create a new vLLM instance with random instance ID"""

    try:
        result = vllm_manager.create_instance(vllm_config)
        return JSONResponse(content=result, status_code=HTTPStatus.CREATED)
    except Exception as e:
        logger.error(f"Failed to create vLLM instance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/v2/vllm/instances/{instance_id}")
async def create_id_vllm_instance(
    vllm_config: VllmConfig,
    instance_id: str = Path(..., description="Custom instance ID"),
):
    """Create a new vLLM instance with instance ID"""
    try:
        result = vllm_manager.create_instance(vllm_config, instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.CREATED)
    except ValueError as e:
        raise HTTPException(status_code=409, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to create vLLM instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v2/vllm/instances/{instance_id}")
async def delete_vllm_instance(
    instance_id: str = Path(..., description="Instance ID to delete")
):
    """Delete a specific vLLM instance"""
    try:
        result = vllm_manager.stop_instance(instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")
    except Exception as e:
        logger.error(f"Failed to delete vLLM instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v2/vllm/instances")
async def delete_all_vllm_instances():
    """Delete all vLLM instances"""
    try:
        result = vllm_manager.stop_all_instances()
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except Exception as e:
        logger.error(f"Failed to delete all vLLM instances: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v2/vllm/instances")
async def get_all_vllm_instances(detail: bool = True):
    """
    Get information about all vLLM instances

    Query Parameters:
    - detail: If True (default), returns full status of all instances.
              If False, returns only instance IDs.
    """
    if detail:
        result = vllm_manager.get_all_instances_status()
    else:
        instances = vllm_manager.list_instances()
        result = {"instance_ids": instances, "count": len(instances)}

    return JSONResponse(content=result, status_code=HTTPStatus.OK)


@app.get("/v2/vllm/instances/{instance_id}")
async def get_vllm_instance_status(
    instance_id: str = Path(..., description="Instance ID")
):
    """Get status of a specific vLLM instance"""
    try:
        result = vllm_manager.get_instance_status(instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")


@app.get("/v2/vllm/instances/{instance_id}/log")
async def get_vllm_instance_logs(
    instance_id: str = Path(..., description="Instance ID"),
    start_byte: int = Query(
        0,
        description="Byte position to start reading from (0-based)",
        ge=0,
    ),
    max_bytes: int = Query(
        MAX_LOG_RESPONSE_BYTES,
        description="Maximum bytes of log data to retrieve",
        ge=1024,
        le=10 * 1024 * 1024,
    ),
):
    """
    Get logs from a specific vLLM instance starting from a byte position.

    Use start_byte=0 to read from the beginning.
    Use the returned next_byte value in subsequent requests to continue reading.
    """
    try:
        log_content, next_byte = vllm_manager.get_instance_logs(
            instance_id, start_byte, max_bytes
        )
        total_bytes = len(log_content.encode("utf-8"))
        return JSONResponse(
            content={
                "instance_id": instance_id,
                "log": log_content,
                "start_byte": start_byte,
                "total_bytes": total_bytes,
                "next_byte": next_byte,
            },
            status_code=HTTPStatus.OK,
        )
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")
    except Exception as e:
        logger.error(f"Failed to get logs for instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


######################################################################
# HELPER FUNCTIONS
######################################################################


# Helper class to redirect stdout/stderr to queue
class QueueWriter:
    """Custom writer that sends output to a multiprocessing Queue"""

    def __init__(self, queue: multiprocessing.Queue):
        self.queue = queue

    def write(self, msg: str):
        if msg.strip():  # Only send non-empty messages
            try:
                self.queue.put_nowait(msg)
            except queue.Full:
                # Drop message if queue is full
                pass

    def flush(self):
        pass


# Function to be executed by the child process
def vllm_kickoff(vllm_config: VllmConfig, output_queue: multiprocessing.Queue):
    """
    Child function to kickoff vllm instance
    :param vllm_config: vLLM configuration parameters and env variables
    :param output_queue: multiprocessing Queue for capturing stdout/stderr
    """

    # Redirect stdout and stderr to queue
    sys.stdout = QueueWriter(output_queue)
    sys.stderr = QueueWriter(output_queue)

    logger.info(f"VLLM process (PID: {os.getpid()}) started.")
    # Set env vars in the current process
    if vllm_config.env_vars:
        set_env_vars(vllm_config.env_vars)

    # prepare args
    receive_args = vllm_config.options.split()

    cli_env_setup()
    parser = FlexibleArgumentParser(
        description="vLLM OpenAI-Compatible RESTful API server."
    )
    parser = make_arg_parser(parser)
    args = parser.parse_args(receive_args)
    validate_parsed_serve_args(args)

    uvloop.run(run_server(args))


# Function to set env variables
def set_env_vars(env_vars: Dict[str, Any]):
    """
    Set environment variables from a dictionary
    :param env_vars: Dict with environment var name as keys and value as values
    """

    # Set environment variables from a dictionary
    for key, value in env_vars.items():
        os.environ[key] = str(value)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info")
