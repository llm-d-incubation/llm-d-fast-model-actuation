# Copyright 2025 The llm-d Authors.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# 	http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
vLLM Launcher
"""

import logging
import multiprocessing
import os
import uuid
from http import HTTPStatus  # HTTP Status Codes
from typing import Any, Dict, List, Optional

import uvloop
from fastapi import FastAPI, HTTPException, Path
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from vllm.entrypoints.openai.api_server import run_server
from vllm.entrypoints.openai.cli_args import make_arg_parser, validate_parsed_serve_args
from vllm.entrypoints.utils import cli_env_setup
from vllm.utils import FlexibleArgumentParser


# Define a the expected JSON structure in dataclass
class VllmConfig(BaseModel):
    options: str
    env_vars: Optional[Dict[str, Any]] = None


class VllmInstance:
    """Represents a single vLLM instance"""

    def __init__(self, instance_id: str, config: VllmConfig):
        """_summary_

        :param instance_id: Instance id (autogenerated or custom)
        :param config: VllmConfig object
        """
        self.instance_id = instance_id
        self.config = config
        self.process: Optional[multiprocessing.Process] = None

    def start(self) -> dict:
        """
        Start this vLLM instance
        :return: Status of the process and its PID.
        """
        if self.process and self.process.is_alive():
            return {"status": "already_running", "instance_id": self.instance_id}

        self.process = multiprocessing.Process(target=vllm_kickoff, args=(self.config,))
        self.process.start()

        return {
            "status": "started",
            "instance_id": self.instance_id,
            "pid": self.process.pid,
        }

    def stop(self, timeout: int = 10) -> dict:
        """
        Stop existing vLLM instance
        :param timeout: waits for the process to stop, defaults to 10
        :return: a dictionary with the status "terminated" and the process ID
        """
        if not self.process or not self.process.is_alive():
            return {
                "status": "not_running",
                "instance_id": self.instance_id,
            }

        pid = self.process.pid

        # Graceful termination
        self.process.terminate()
        self.process.join(timeout=timeout)

        # Force kill if needed
        if self.process.is_alive():
            self.process.kill()
            self.process.join()

        return {
            "status": "terminated",
            "instance_id": self.instance_id,
            "pid": pid,
        }

    def is_running(self) -> bool:
        """
        Returns if the process in the manager is running or not.
        :return: True is running, `False` otherwise.
        """
        return self.process is not None and self.process.is_alive()

    def get_status(self) -> dict:
        """
        Returns the status of the process and its PID or the no process
        :return: Status and PID of the running process.
        """
        if not self.process:
            return {
                "status": "not_started",
                "instance_id": self.instance_id,
                "pid": None,
            }

        return {
            "status": "running" if self.process.is_alive() else "stopped",
            "instance_id": self.instance_id,
            "pid": self.process.pid,
        }


# Multi-instance vLLM process manager
class VllmMultiProcessManager:
    def __init__(self):
        self.instances: Dict[str, VllmInstance] = {}

    def create_instance(
        self, vllm_config: VllmConfig, instance_id: Optional[str] = None
    ) -> dict:
        """Create and start a new vLLM instance"""
        if instance_id is None:
            instance_id = str(uuid.uuid4())

        if instance_id in self.instances:
            raise ValueError(f"Instance with ID {instance_id} already exists")

        instance = VllmInstance(instance_id, vllm_config)
        self.instances[instance_id] = instance

        return instance.start()

    def stop_instance(self, instance_id: str, timeout: int = 10) -> dict:
        """Stop a specific vLLM instance"""
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")

        result = self.instances[instance_id].stop(timeout)

        # Clean up stopped instance
        if result["status"] in ["terminated", "not_running"]:
            del self.instances[instance_id]

        return result

    def stop_all_instances(self, timeout: int = 10) -> dict:
        """Stop all running vLLM instances"""
        results = []
        instance_ids = list(self.instances.keys())

        for instance_id in instance_ids:
            try:
                result = self.stop_instance(instance_id, timeout)
                results.append(result)
            except KeyError:
                continue  # Instance was already removed

        return {
            "status": "all_stopped",
            "stopped_instances": results,
            "total_stopped": len(results),
        }

    def get_instance_status(self, instance_id: str) -> dict:
        """Get status of a specific instance"""
        if instance_id not in self.instances:
            raise KeyError(f"Instance {instance_id} not found")

        return self.instances[instance_id].get_status()

    def get_all_instances_status(self) -> dict:
        """Get status of all instances"""
        instances_status = []
        running_count = 0

        for instance in self.instances.values():
            status = instance.get_status()
            instances_status.append(status)
            if status["status"] == "running":
                running_count += 1

        return {
            "total_instances": len(self.instances),
            "running_instances": running_count,
            "instances": instances_status,
        }

    def list_instances(self) -> List[str]:
        """List all instance IDs"""
        return list(self.instances.keys())


# Create global manager instance
vllm_manager = VllmMultiProcessManager()

# Create FastAPI application
app = FastAPI(
    title="Multi-Instance vLLM Management API",
    version="2.0",
    description="REST API for managing multiple vLLM instances",
)

# Setup logging
logger = logging.getLogger(__name__)


############################################################
# Health Endpoint
############################################################
@app.get("/health")
async def health():
    """Health Status"""
    return JSONResponse(content={"status": "OK"}, status_code=HTTPStatus.OK)


######################################################################
# GET INDEX
######################################################################
@app.get("/")
async def index():
    """Root URL response"""
    return JSONResponse(
        content={
            "name": "Multi-Instance vLLM Management API",
            "version": "2.0",
            "endpoints": {
                "create_instance": "POST /v2/vllm/instances",
                "create_named_instance": "PUT /v2/vllm/instances/{instance_id}",
                "delete_instance": "DELETE /v2/vllm/instances/{instance_id}",
                "delete_all_instances": "DELETE /v2/vllm/instances",
                "get_instance_status": "GET /v2/vllm/instances/{instance_id}",
                "get_all_instances": "GET /v2/vllm/instances",
            },
        },
        status_code=HTTPStatus.OK,
    )


######################################################################
# vLLM MANAGEMENT ENDPOINTS
######################################################################
@app.post("/v2/vllm/instances")
async def create_vllm_instance(vllm_config: VllmConfig):
    """Create a new vLLM instance with random instance ID"""

    try:
        result = vllm_manager.create_instance(vllm_config)
        return JSONResponse(content=result, status_code=HTTPStatus.CREATED)
    except Exception as e:
        logger.error(f"Failed to create vLLM instance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/v2/vllm/instances/{instance_id}")
async def create_id_vllm_instance(
    vllm_config: VllmConfig,
    instance_id: str = Path(..., description="Custom instance ID"),
):
    """Create a new vLLM instance with instance ID"""
    try:
        result = vllm_manager.create_instance(vllm_config, instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.CREATED)
    except ValueError as e:
        raise HTTPException(status_code=409, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to create vLLM instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v2/vllm/instances/{instance_id}")
async def delete_vllm_instance(
    instance_id: str = Path(..., description="Instance ID to delete")
):
    """Delete a specific vLLM instance"""
    try:
        result = vllm_manager.stop_instance(instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")
    except Exception as e:
        logger.error(f"Failed to delete vLLM instance {instance_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v2/vllm/instances")
async def delete_all_vllm_instances():
    """Delete all vLLM instances"""
    try:
        result = vllm_manager.stop_all_instances()
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except Exception as e:
        logger.error(f"Failed to delete all vLLM instances: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v2/vllm/instances")
async def get_all_vllm_instances(detail: bool = True):
    """
    Get information about all vLLM instances

    Query Parameters:
    - detail: If True (default), returns full status of all instances.
              If False, returns only instance IDs.
    """
    if detail:
        result = vllm_manager.get_all_instances_status()
    else:
        instances = vllm_manager.list_instances()
        result = {"instance_ids": instances, "count": len(instances)}

    return JSONResponse(content=result, status_code=HTTPStatus.OK)


@app.get("/v2/vllm/instances/{instance_id}")
async def get_vllm_instance_status(
    instance_id: str = Path(..., description="Instance ID")
):
    """Get status of a specific vLLM instance"""
    try:
        result = vllm_manager.get_instance_status(instance_id)
        return JSONResponse(content=result, status_code=HTTPStatus.OK)
    except KeyError:
        raise HTTPException(status_code=404, detail=f"Instance {instance_id} not found")


######################################################################
# HELPER FUNCTIONS
######################################################################


# Function to be executed by the child process
def vllm_kickoff(vllm_config: VllmConfig):
    """
    Child function to kickoff vllm instance
    :param vllm_config: vLLM configuration parameters and env variables
    """

    logger.info(f"VLLM process (PID: {os.getpid()}) started.")
    # Set env vars in the current process
    if vllm_config.env_vars:
        set_env_vars(vllm_config.env_vars)

    # prepare args
    receive_args = vllm_config.options.split()

    cli_env_setup()
    parser = FlexibleArgumentParser(
        description="vLLM OpenAI-Compatible RESTful API server."
    )
    parser = make_arg_parser(parser)
    args = parser.parse_args(receive_args)
    validate_parsed_serve_args(args)

    uvloop.run(run_server(args))


# Function to set env variables
def set_env_vars(env_vars: Dict[str, Any]):
    """
    Set environment variables from a dictionary
    :param env_vars: Dict with environment var name as keys and value as values
    """
    # Set environment variables from a dictionary
    for key, value in env_vars.items():
        os.environ[key] = str(value)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info")
